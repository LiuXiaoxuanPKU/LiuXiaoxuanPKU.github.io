## About Me

Hi, I am Lily (Xiaoxuan) Liu. I am a CS PhD student from UC Berkeley. I am affiliated with [Sky Lab](https://sky.cs.berkeley.edu/people/) (formerly known as RISE/AMP Lab). I am fortunately advised by Professor [Alvin Cheung](https://people.eecs.berkeley.edu/~akcheung/) and Professor [Ion Stoica](http://people.eecs.berkeley.edu/~istoica/). I got my master's from CMU, working with amazing [Andy](http://www.cs.cmu.edu/~pavlo/) and [Huanchen](http://www.cs.cmu.edu/~huanche1/). I did my undergraduate in PKU. 

### Research Interest
I am broadly interested in database and machine learning system research. Concretely, I am interested in building fast, cost-efficient LLM systems. I am fortunate to learn from my colleagues and become part of the [vllm](https://github.com/vllm-project/vllm) team.

### Selected Publication and Manuscripts
- **[Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066))**, **arxiv**, 2024. \
**Xiaoxuan Liu**, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang
- **[MÃ©lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity](https://arxiv.org/abs/2404.14527)**, **arxiv**, 2024. [[code](https://github.com/tyler-griggs/melange-release)] \
  Tyler Griggs<sup>*</sup>, **Xiaoxuan Liu**<sup>*</sup>, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica
- **[Online Speculative Decoding](https://arxiv.org/abs/2310.07177)**, **ICML 2024**. [[code](https://github.com/LiuXiaoxuanPKU/OSD)] \
**Xiaoxuan Liu**, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang
- **[Leveraging Application Data Constraints to Optimize Database-Backed Web Applications](https://arxiv.org/abs/2205.02954)**, **VLDB 2023**. [[code](https://github.com/LiuXiaoxuanPKU/ConstrOpt)] \
  **Xiaoxuan Liu**, Shuxian Wang, Mengzhu Sun, Sicheng Pan, Ge Li, Siddharth Jha, Cong Yan, Junwen Yang, Shan Lu, Alvin Cheung.
- **[GACT: Activation Compressed Training for Generic Network Architectures](https://arxiv.org/abs/2206.11357)**, **ICML 2022**. [[code](https://github.com/LiuXiaoxuanPKU/GACT-ICML)][[slides](https://github.com/LiuXiaoxuanPKU/LiuXiaoxuanPKU.github.io/blob/master/docs/slides/ICML2022.pdf)][[talk](https://slideslive.com/38983883/gact-activation-compressed-training-for-generic-network-architectures)] \
  **Xiaoxuan Liu**, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung.
- **[Order-Preserving Key Compression for In-Memory Search Trees](https://arxiv.org/abs/2003.02391)**, **SIGMOD 2020**. [[code](https://github.com/efficient/HOPE)][[slides](http://people.iiis.tsinghua.edu.cn/~huanchen/slides/hope-sigmod20.pdf)][[talk](https://www.youtube.com/watch?v=9OzjeSbWHcQ)]\
  Huanchen Zhang, **Xiaoxuan Liu**, David Anderson, Michael Kaminsky, Kimberly Keeton, Andrew Pavlo.

#### Papers that ~~die silently~~ I do randomly
- **[An Evaluation of Memory Optimization Methods for Training Neural Networks](https://arxiv.org/abs/2303.14633)**, **arxiv**, 2023 \
  **Xiaoxuan Liu**, Siddharth Jha, Alvin Cheung
